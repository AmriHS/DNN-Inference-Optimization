# DNN Inference Optimization
## The goals of this project are:
- Exploring the configuration space from hardware, compilar, environment-level parameters for Machine Learning algorithem, Deep Neural Network.
- How changing in parameters influence the inference time and power consumption of Deep Neural Network. 
- Hiring Multi-objective optimization to find the optimal configuration space to minmize the power consumption and inference time, and the optimial trade-off between them.
- Constuct an informative model to inference about the performance of Deep Neural network given configuration space.
- The interaction of configuration parameters using polynomial regression.
- Prediction using Gradiant-boosting regression.

## Result:
![alt text](/result/40_samples.png)


Written using:
- Python both 3.6 and 2.7
- Shell script
