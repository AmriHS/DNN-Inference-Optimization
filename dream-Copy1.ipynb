{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMhGdYHuOZM8"
   },
   "source": [
    "# Design Space Exploration of Deep Neural Network in Resource Constrained Devices\n",
    "\n",
    "Credits: (https://github.com/AmriHS/DNN-Inference-Optimization) \n",
    "by Yang Ren, Rui Xin, Hassan Alamri\n",
    "\n",
    "This notebook demonstrates how to use our framework to produce the  in the final report. \n",
    "\n",
    "The environment set up as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab_type": "code",
    "id": "Pqz5k4syOZNA"
   },
   "outputs": [],
   "source": [
    "# imports and basic setup for SVR_prediction.py\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# imports and basic setup for baysian_opt.py\n",
    "import paramiko\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import gpflow\n",
    "import gpflowopt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "from gpflowopt.acquisition import ExpectedImprovement\n",
    "from random import randint\n",
    "\n",
    "# imports and basic setup for regression_poly.py\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# imports and basic setup for run_bench_v2.py\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import keras\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import Keras_Resnet50 as res50\n",
    "import subprocess\n",
    "import time\n",
    "import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeF9mG-COZNE"
   },
   "source": [
    "## Shell File Introduction\n",
    "In our project, we has four shell files to change the GPU factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cpu_freq.sh file to change the cpu frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab_type": "code",
    "id": "i9hkSm1IOZNR"
   },
   "outputs": [],
   "source": [
    "num_cores=$1\n",
    "cpu_freq=$2\n",
    "cur_freq=$(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq)\n",
    "\n",
    "# Disable/enable CPU cores\n",
    "# 3 is the total number of cores we are able to enable.\n",
    "# We start from core 1 to num_cores requested to disable. Subsequently, we enable cores that are not asked for. \n",
    "\n",
    "for i in $(seq 1 1 $num_cores)\n",
    "\tdo\n",
    "\t\tsudo bash -c 'echo 0 > /sys/devices/system/cpu/cpu['$i']/online'\n",
    "\tdone\n",
    "\n",
    "num_cores=$((num_cores+1))\n",
    "for i in $(seq $num_cores 1 3) \n",
    "\tdo\n",
    "\t\tsudo bash -c 'echo 1 > /sys/devices/system/cpu/cpu['$i']/online'\n",
    "\tdone\n",
    "\n",
    "\n",
    "#Change GPU Frequency\n",
    "if [ ! -z $cpu_freq ];\n",
    "then\n",
    "\tif [ $cpu_freq -gt $cur_freq ];\n",
    "\tthen\n",
    "\t\tsudo bash -c 'echo '${cpu_freq}' > /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq'\n",
    "\t\tsudo bash -c 'echo '${cpu_freq}' > /sys/devices/system/cpu/cpu0/cpufreq/scaling_min_freq'\n",
    "\t\t\n",
    "\telse\n",
    "\t\tsudo bash -c 'echo '${cpu_freq}' > /sys/devices/system/cpu/cpu0/cpufreq/scaling_min_freq'\n",
    "\t\tsudo bash -c 'echo '${cpu_freq}' > /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq'\n",
    "\tfi\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use emc_freq.sh file to change the emc frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emc_freq=$1\n",
    "cur_freq=$(cat /sys/kernel/debug/clk/override.emc/clk_rate)\n",
    "\n",
    "#Change EMC Frequency\n",
    "sudo bash -c 'echo '${emc_freq}' > /sys/kernel/debug/clk/override.emc/clk_update_rate'\n",
    "sudo bash -c 'echo 1 > /sys/kernel/debug/clk/override.emc/clk_state'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gpu_freq.sh file to change the GPU frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_freq=$1\n",
    "cur_freq=$(cat /sys/devices/57000000.gpu/devfreq/57000000.gpu/cur_freq)\n",
    "\n",
    "#Change GPU Frequency\n",
    "if [ $gpu_freq -gt $cur_freq ];\n",
    "then\n",
    "\tsudo bash -c 'echo '${gpu_freq}' >  /sys/devices/57000000.gpu/devfreq/57000000.gpu/max_freq'\n",
    "\tsudo bash -c 'echo '${gpu_freq}' >  /sys/devices/57000000.gpu/devfreq/57000000.gpu/min_freq'\n",
    "else \n",
    "\tsudo bash -c 'echo '${gpu_freq}' >  /sys/devices/57000000.gpu/devfreq/57000000.gpu/min_freq'\n",
    "\tsudo bash -c 'echo '${gpu_freq}' >  /sys/devices/57000000.gpu/devfreq/57000000.gpu/max_freq'\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use script.sh file to set the configuration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible configuration space\n",
    "$CPU_FREQ = $1\n",
    "$CPU_DIS_CORES = $2\n",
    "$GPU_FREQ = $3\n",
    "$EMC_FREQ = $4\n",
    "\n",
    "sh ./cpu_freq.sh $CPU_FREQ $CPU_DIS_CORES\n",
    "sh ./gpu_freq.sh $GPU_FREQ\n",
    "sh ./emc_freq.sh $EMC_FREQ\n",
    "\n",
    "python run_benchmark.py --bsize 32 --all_growth 1 --mem_frac 0.25\n",
    "\n",
    "# verify configuration\n",
    "cur_gpu_freq=$(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq)\n",
    "cur_cpu_freq=$(cat /sys/devices/57000000.gpu/devfreq/57000000.gpu/cur_freq)\n",
    "cur_emc_freq=$(cat /sys/kernel/debug/tegra_bwmgr/emc_rate)\n",
    "dis_cpu_core_1=$(cat /sys/devices/system/cpu/cpu1/online)\n",
    "dis_cpu_core_2=$(cat /sys/devices/system/cpu/cpu2/online)\n",
    "dis_cpu_core_3=$(cat /sys/devices/system/cpu/cpu3/online)\n",
    "\n",
    "\n",
    "echo \"GPU Frequency: ${cur_gpu_freq}\"\n",
    "echo \"CPU Frequency: ${cur_cpu_freq}\"\n",
    "echo \"EMC Frequency: ${cur_emc_freq}\"\n",
    "echo \"CPU 1 core Status: ${dis_cpu_core_1}\"\n",
    "echo \"CPU 2 core Status: ${dis_cpu_core_2}\"\n",
    "echo \"CPU 3 core Status: ${dis_cpu_core_3}\"\n",
    "#echo \"$host, `date`, checkout,$Time_checkout\" >> log.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeV_fJ4QOZNb"
   },
   "source": [
    "##  Pre-Train Model Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we use two pre-train model as the input model:\n",
    "    * Keras_Resnet50\n",
    "    * VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras_Resnet50.py\n",
    "# An example using Keras Resnet50 pre-trained model to measure the inference time\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "#import apscheduler.schedulers.blocking\n",
    "import commands\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from timeit import default_timer as timer\n",
    "from keras.datasets import cifar10\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "import keras.backend.tensorflow_backend as ktf\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 #, os\n",
    "import csv\n",
    "import gc \n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import subprocess\n",
    "\n",
    "import logging\n",
    "\n",
    "#log = logging.getLogger('apscheduler.executors.default')\n",
    "#log.setLevel(logging.INFO)  # DEBUG\n",
    "\n",
    "#fmt = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "#h = logging.StreamHandler()\n",
    "#h.setFormatter(fmt)\n",
    "#log.addHandler(h)\n",
    "\n",
    "\n",
    "logger = logging.getLogger()  # this returns the root logger\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "Time = []\n",
    "power_cons = []\n",
    "\n",
    "def tick(): \n",
    "    # Read current power consumptions\n",
    "    input0 = open('/sys/devices/7000c400.i2c/i2c-1/1-0040/iio_device/in_power0_input', 'r')\n",
    "    input1 = open('/sys/devices/7000c400.i2c/i2c-1/1-0040/iio_device/in_power1_input', 'r')\n",
    "    input2 = open('/sys/devices/7000c400.i2c/i2c-1/1-0040/iio_device/in_power2_input', 'r')\n",
    "    mod_power = input0.readline()\n",
    "    gpu_power = input1.readline()\n",
    "    cpu_power = input2.readline()\n",
    "    power_cons.append([float(mod_power), float(gpu_power), float(cpu_power)])\n",
    "    CurrentTime = time.time()\n",
    "    Time.append(int(CurrentTime))\n",
    "    #print('Tick! The time is: %s' % datetime.now())\n",
    "\n",
    "def RelationPlot(Time):\n",
    "    #plt.plot(Time, PowerConsumption)\n",
    "    #plt.xlabel('Time')\n",
    "    #plt.ylabel('PowerConsumption')\n",
    "    #plt.savefig(\"PowerConsumption_test_timer.jpg\")\n",
    "    len_power=len(power_cons)\n",
    "    mod_power_sum = 0\n",
    "    gpu_power_sum = 0\n",
    "    cpu_power_sum = 0\n",
    "    for i in range(len_power):\n",
    "        #print(\"Current Power Consumption:\"+repr(power_cons[i]))\n",
    "        mod_power_sum+=power_cons[i][0]\n",
    "        gpu_power_sum+=power_cons[i][1]\n",
    "        cpu_power_sum+=power_cons[i][2]\n",
    "    mod_power_sum/=len_power\n",
    "    gpu_power_sum/=len_power\n",
    "    cpu_power_sum/=len_power\n",
    "    return [mod_power_sum, gpu_power_sum, cpu_power_sum]\n",
    "\n",
    "def write_to_csv(data, filename):\n",
    "    with open(filename,'w') as out:\n",
    "        csv_out= csv.writer(out, lineterminator='\\n')\n",
    "        csv_out.writerow(['Class', 'Prob'])\n",
    "        for row in data:\n",
    "            csv_out.writerow(row[0][1:])\n",
    "\n",
    "def resize(dataset):\n",
    "    processed_data = []\n",
    "    for i in range(len(dataset)):\n",
    "        x = cv2.resize(dataset[i], (224,224))\n",
    "        x = image.img_to_array(x)\n",
    "        #x = np.expand_dims(x, axis=0)\n",
    "        processed_data.append(preprocess_input(x))\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def make_predictions(dataset, batch_size, allow_growth, memory_frac):\n",
    "    #print('getting into models!')\n",
    "    # reset values of power consumption\n",
    "    power_cons = []\n",
    "\n",
    "    scheduler = BackgroundScheduler()\n",
    "    #scheduler = apscheduler.schedulers.blocking.BackgroundScheduler('apscheduler.job_defaults.max_instances': '2')\n",
    "    #print('BackgroundScheduler define')\n",
    "    scheduler.add_job(tick, 'interval', seconds=0.5, misfire_grace_time=1)# execute every 0.5 second\n",
    "    #print('job added!')\n",
    "    config = tf.ConfigProto(log_device_placement=False, device_count = {'GPU' :1})\n",
    "    if allow_growth:\n",
    "        config.gpu_options.allow_growth = True\n",
    "    else:\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = memory_frac\n",
    "\n",
    "    session = tf.Session(config=config)\n",
    "    ktf.set_session(session)\n",
    "\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    # start time\n",
    "    try:\n",
    "        scheduler.start()# new seperate thread\n",
    "        #print('Press Ctrl+{0} to exit'.format('Break' if os.name == 'nt' else 'C'))\n",
    "        start = timer()\n",
    "        preds = model.predict(np.array(dataset), batch_size=batch_size)\n",
    "        # end time\n",
    "        end = timer()\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        # Not strictly necessary if daemonic mode is enabled but should be done if possible\n",
    "        scheduler.shutdown()\n",
    "    scheduler.shutdown()\n",
    "    #ktf.clear_session()\n",
    "    session.close()\n",
    "    del session\n",
    "    gc.collect()\n",
    "    power_cons = RelationPlot(Time)\n",
    "    # calculate runtime\n",
    "    runtime = end-start\n",
    "    #print('Runtime: ' + \"{0:.2f}\".format(runtime) + 's')\n",
    "    return preds, runtime, power_cons\n",
    "\n",
    "def run_resnet50_benchmark(dataset, batch_size, all_growth=True, mem_frac=None):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"   \n",
    "    preds, runtime, power_cons = make_predictions(dataset[:100], batch_size, all_growth, mem_frac)\n",
    "    decoded =  decode_predictions(preds, top=1)\n",
    "    write_to_csv(decoded, \"Keras_result.csv\")\n",
    "    #data = [runtime, power_cons[0],power_cons[1], power_cons[2]]\n",
    "    return runtime, power_cons \n",
    "    #for i in range(len(data)):\n",
    "    #\tq.put(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Strategy\n",
    "In our project, we use baysian optimization, the reson is:\n",
    "* Black-box optimization\n",
    "* Small number of function evaluations\n",
    "* Exploit regions that yield good points\n",
    "* And explore regions with high uncertainty\n",
    "* With small number of evaluations, it builds an informative model \n",
    "* Based on Gaussian Process (GP)\n",
    "** Uses previously observed parameters to make an assumption about unobserved parameters.  \n",
    "* Acquisition Function used to intelligently suggest the next set of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baysian_opt.py will execute the baysian optimization process, the file detail as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import gpflow\n",
    "import gpflowopt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "from gpflowopt.acquisition import ExpectedImprovement\n",
    "from random import randint\n",
    "random.seed(24)\n",
    "\n",
    "# host IP and credential\n",
    "ip='10.173.131.120'\n",
    "port=22\n",
    "username='ubuntu'\n",
    "password='ubuntu'\n",
    "\n",
    "#Linux commands\n",
    "file_dir = 'cd DNN_Inference;'\n",
    "sudo_cmd = 'sudo -S '\n",
    "command='python run_bench_v2.py --cpu_freq {0} --num_cores {1} --gpu_freq {2} --emc_freq {3} --bsize {4}' \\\n",
    "    +' --all_growth {5} --mem_frac {6}'\n",
    "command2='python DNN_Inference/ssh_ex.py'\n",
    "\n",
    "# define discrete values of each input space\n",
    "batch_size_array=[1, 8, 16, 32] # 16 & 32 is excluded for VGG16\n",
    "all_growth_array=[True, False]\n",
    "memory_frac_array=[0.15, 0.2, 0.25, 0.3, 0.33] # 0.15, 0.2,  is excluded for VGG16 model\n",
    "GPU_freq_array=[76800000,153600000, 230400000, 307200000, 384000000, 460800000, 537600000, 614400000,\n",
    "                691200000, 768000000, 844800000, 921600000, 998400000, 537600000,998400000]\n",
    "CPU_freq_array=[102000,204000,306000, 408000, 510000, 612000, 714000, 816000, 918000,1020000,1122000, 1224000,\n",
    "                1326000,1224000,1428000,1555000,1632000, 1734000]\n",
    "\n",
    "# 40800000 frequency excluded due to frequent memory issue\n",
    "EMC_freq_array=[800000000,1065600000,1331200000,1600000000]\n",
    "num_dis_core_array=[0,1,2,3]\n",
    "\n",
    "# define space input, upper and lower bounds for each dimension\n",
    "domain = gpflowopt.domain.ContinuousParameter('CPU_frequency', 102000, 1734000) + \\\n",
    "         gpflowopt.domain.ContinuousParameter('num_cores_dis', 0, 4) + \\\n",
    "         gpflowopt.domain.ContinuousParameter('GPU_frequency', 76800000, 998400000) + \\\n",
    "         gpflowopt.domain.ContinuousParameter('EMC_frequency', 800000000, 1600000000)+ \\\n",
    "         gpflowopt.domain.ContinuousParameter('Batch_size', 1, 32) + \\\n",
    "         gpflowopt.domain.ContinuousParameter('Allow_growth', 0, 1) + \\\n",
    "         gpflowopt.domain.ContinuousParameter('Memory_fraction', 0.15, 0.33)\n",
    "\n",
    "is_sampling = True\n",
    "\n",
    "# connect to linux server using ssh\n",
    "ssh=paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect(ip,port,username,password)\n",
    "\n",
    "# write output to csv file\n",
    "def write_to_csv(data, filename):\n",
    "    with open(filename,'a') as out:\n",
    "        csv_out= csv.writer(out, lineterminator='\\n')\n",
    "        #csv_out.writerow(['CPU Frequency', '# of enabled cores','GPU Frequency','EMC Frequency','Batch Size', 'Mem Growth',\n",
    "        #                  'Mem Fraction', 'Runtime', 'Model Cons','GPU Cons', 'CPU Cons'])\n",
    "        for row in data:\n",
    "            csv_out.writerow(row)\n",
    "\n",
    "# find closest discrete values to the continuous input\n",
    "def find_closest(values):\n",
    "    values[0] = min(CPU_freq_array, key=lambda x:abs(x-values[0]))\n",
    "    values[1] = min(num_dis_core_array, key=lambda x:abs(x-values[1]))\n",
    "    values[2] = min(GPU_freq_array, key=lambda x:abs(x-values[2]))\n",
    "    values[3] = min(EMC_freq_array, key=lambda x:abs(x-values[3]))\n",
    "    values[4] = min(batch_size_array, key=lambda x:abs(x-values[4]))\n",
    "    values[5] = min(all_growth_array, key=lambda x:abs(x-values[5]))\n",
    "    values[6] = min(memory_frac_array, key=lambda x:abs(x-values[6]))\n",
    "\n",
    "\n",
    "def handle_output(output):\n",
    "    if len(output) == 0:\n",
    "        return [None, None, None, None]\n",
    "    else:\n",
    "        return [float(output[0]), float(output[1]), float(output[2]), float(output[3])]\n",
    "\n",
    "# Optimization multo-objective function\n",
    "def objective_func(params):\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    benchmark_data = []\n",
    "    for i in range(len(params)):\n",
    "        print (\"Iteration:\"+str(i))\n",
    "        find_closest(params[i])\n",
    "        print (params[i,1])\n",
    "        cmd = command.format(int(params[i,0]), int(params[i,1]),int(params[i,2]),int(params[i,3]),int(params[i,4]),\n",
    "                             int(params[i,5]),float(params[i,6]))\n",
    "        print (\"Command:\"+repr(cmd))\n",
    "        stdin,stdout,stderr=ssh.exec_command(file_dir+sudo_cmd+cmd)\n",
    "        stdin.write(\"ubuntu\\n\")\n",
    "        stdin.flush()\n",
    "        outlines=stdout.readlines()\n",
    "        response=''.join(outlines).split()\n",
    "        response = handle_output(response)\n",
    "        print (response)\n",
    "        benchmark_data.append([params[i,0], params[i,1], params[i,2], params[i,3], params[i,4], params[i,5],\n",
    "                         params[i,6], response[0], response[1], response[2], response[3]])\n",
    "        y1.append([response[0]])\n",
    "        y2.append([response[1]])\n",
    "    write_to_csv(benchmark_data, \"result.csv\")\n",
    "    return np.hstack((y1,y2))\n",
    "\n",
    "def random_search():\n",
    "    n_samples = 40\n",
    "    design = gpflowopt.design.RandomDesign(n_samples, domain)\n",
    "    X = design.generate()\n",
    "    Y = objective_func(X)\n",
    "    itemindex = np.where(Y[:n_samples]==None) #discard samples where it doesn't produce output\n",
    "    Y = np.delete(Y, (itemindex[0]), axis=0)\n",
    "    Y = np.array(Y, dtype=float)\n",
    "\n",
    "    plt.scatter(Y[:,0], Y[:,1])\n",
    "    plt.title('Random set')\n",
    "    plt.xlabel('Inference Time')\n",
    "    plt.ylabel('Power Consumption')\n",
    "    plt.show()\n",
    "\n",
    "    print (Y.shape[0])\n",
    "\n",
    "    plt.plot(np.arange(0, Y.shape[0]),np.minimum.accumulate(Y[:,0]) ,'b',label='Inference Time')\n",
    "    plt.ylabel('fmin')\n",
    "    plt.xlabel('Number of evaluated points')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(0, Y.shape[0]),np.minimum.accumulate(Y[:,1]) ,'g',label='Power Consumption')\n",
    "    plt.ylabel('fmin')\n",
    "    plt.xlabel('Number of evaluated points')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def baysian_opt():\n",
    "    global is_sampling\n",
    "    n_samples = 10\n",
    "    design = gpflowopt.design.LatinHyperCube(n_samples, domain)\n",
    "    X = design.generate()\n",
    "    X = np.array(X)\n",
    "    Y = objective_func(X)\n",
    "    # discard samples where it doesn't produce output\n",
    "    itemindex = np.where(Y[:n_samples]==None)\n",
    "    Y = np.delete(Y, (itemindex[0]), axis=0)\n",
    "    X = np.delete(X, (itemindex[0]), axis=0)\n",
    "    Y = np.array(Y, dtype=float)\n",
    "    n_samples = len(X)\n",
    "    is_sampling = False\n",
    "    # One model for each objective\n",
    "    objective_models = [gpflow.gpr.GPR(X.copy(), Y[:,[i]].copy(), gpflow.kernels.Matern52(domain.size, ARD=True)) for i in range(Y.shape[1])]\n",
    "    for model in objective_models:\n",
    "        model.likelihood.variance = 0.01\n",
    "\n",
    "    hvpoi = gpflowopt.acquisition.HVProbabilityOfImprovement(objective_models)\n",
    "    acquisition_opt = gpflowopt.optim.StagedOptimizer([gpflowopt.optim.MCOptimizer(domain, n_samples),\n",
    "                                                       gpflowopt.optim.SciPyOptimizer(domain)])\n",
    "\n",
    "    # Then run the BayesianOptimizer for 40 iterations\n",
    "    optimizer = gpflowopt.BayesianOptimizer(domain, hvpoi, optimizer=acquisition_opt, verbose=True)\n",
    "    optimizer.optimize(objective_func, n_iter=30)\n",
    "\n",
    "    pf, dom = gpflowopt.pareto.non_dominated_sort(hvpoi.data[1])\n",
    "\n",
    "    plt.scatter(hvpoi.data[1][:,0], hvpoi.data[1][:,1], c=dom)\n",
    "    plt.title('Pareto set')\n",
    "    plt.xlabel('Inference Time')\n",
    "    plt.ylabel('Power Consumption')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(0, hvpoi.data[0].shape[0]),np.minimum.accumulate(hvpoi.data[1][:,0]) ,'b',label='Inference Time')\n",
    "    plt.ylabel('fmin')\n",
    "    plt.xlabel('Number of evaluated points')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(0, hvpoi.data[0].shape[0]),np.minimum.accumulate(hvpoi.data[1][:,1]) ,'g',label='Power Consumption')\n",
    "    plt.ylabel('fmin')\n",
    "    plt.xlabel('Number of evaluated points')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "baysian_opt()\n",
    "random_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR_prediction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def polynomial(data_x, data_y, split, init_degree=2, max_degree = 2):\n",
    "    #data_x = np.array(data_x, dtype=\"float64\")\n",
    "    #data_y = np.array(data_y, dtype=\"float64\")\n",
    "    #data_y = np.round(data_y, decimals=1)\n",
    "\n",
    "    X_train, Y_train = data_x[:split], data_y[:split]\n",
    "    X_test, Y_test = data_x[split:], data_y[split:]\n",
    "\n",
    "    degrees = np.arange(init_degree,max_degree+1)\n",
    "    pred = []\n",
    "    for i in range(len(degrees)):\n",
    "        model = Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=degrees[i])),\n",
    "            ('linreg', LinearRegression(normalize=True))\n",
    "        ])\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_train_pred = model.predict(X_train)\n",
    "        Y_test_pred = model.predict(X_test)\n",
    "        pred.append(Y_test_pred)\n",
    "\n",
    "        intercept = model.named_steps['linreg'].intercept_[0]\n",
    "        coef = model.named_steps['linreg'].coef_[0]\n",
    "        features = model.named_steps['poly'].get_feature_names()\n",
    "        assert(len(coef) == len(features))\n",
    "\n",
    "        estimated_inf_f = '{:+.2f}'.format(intercept)\n",
    "\n",
    "        for j in range(0, len(coef)-1):\n",
    "            if float ('{:+.1f}'.format(coef[j]).replace('\\U00002013', '-')) != 0.0:\n",
    "                estimated_inf_f += ' + {:+.1f} {}'.format(np.round(coef[j], decimals=2), features[j+1])\n",
    "\n",
    "        print (estimated_inf_f)\n",
    "        # plot function we want to learn\n",
    "        rmse_infer = sqrt(mean_squared_error(Y_test_pred[:,0], Y_test[:,0]))\n",
    "        rmse_power = sqrt(mean_squared_error(Y_test_pred[:,1], Y_test[:,1]))\n",
    "\n",
    "        print('Test RMSE for Inference Time: %.3f' % rmse_infer)\n",
    "        print('Test RMSE for Power Consumption: %.3f' % rmse_power)\n",
    "    return pred\n",
    "\n",
    "def test_multi_target_regression(data_x, data_y):\n",
    "    n_train = int (len(data_x)*0.80)\n",
    "    X_train, Y_train = data_x[:n_train], data_y[:n_train]\n",
    "    X_test, Y_test = data_x[n_train:], data_y[n_train:]\n",
    "\n",
    "    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n",
    "    rgr.fit(X_train, Y_train)\n",
    "    Y_train_pred = rgr.predict(X_train)\n",
    "    Y_test_pred = rgr.predict(X_test)\n",
    "\n",
    "    plot_result(Y_train[:,0],Y_test[:,0], Y_train_pred[:,0], Y_test_pred[:,0])\n",
    "    plot_result(Y_train[:,1],Y_test[:,1], Y_train_pred[:,1], Y_test_pred[:,1])\n",
    "\n",
    "    rmse_infer = sqrt(mean_squared_error(Y_test_pred[:,0], Y_test[:,0]))\n",
    "    rmse_power = sqrt(mean_squared_error(Y_test_pred[:,1], Y_test[:,1]))\n",
    "    rmse_train_infer = sqrt(mean_squared_error(Y_train_pred[:,0], Y_train[:,0]))\n",
    "    rmse_train_power = sqrt(mean_squared_error(Y_train_pred[:,1], Y_train[:,1]))\n",
    "\n",
    "    print('Train RMSE for Inference Time: %.3f' % rmse_train_infer)\n",
    "    print('Train RMSE for Power Consumption: %.3f' % rmse_train_power)\n",
    "    print('Test RMSE for Inference Time: %.3f' % rmse_infer)\n",
    "    print('Test RMSE for Power Consumption: %.3f' % rmse_power)\n",
    "\n",
    "\n",
    "def plot_min_iteration(bay_y, rand_y):\n",
    "    assert (len(bay_y) == len(rand_y))\n",
    "    plt.plot(np.arange(0, bay_y.shape[0]),np.minimum.accumulate(bay_y[:,0]) ,'darkslateblue',label='Baysian Opt Inference')\n",
    "    plt.plot(np.arange(0, rand_y.shape[0]),np.minimum.accumulate(rand_y[:,0]) ,'darkseagreen',label='Random Inference')\n",
    "    plt.ylabel('fmin')\n",
    "    plt.xlabel('Number of evaluated points')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(0, bay_y.shape[0]),np.minimum.accumulate(bay_y[:,1]) ,'firebrick',label='Baysian Opt Power Consumption')\n",
    "    plt.plot(np.arange(0, rand_y.shape[0]),np.minimum.accumulate(rand_y[:,1]) ,'royalblue',label='Random Power Consumption')\n",
    "    plt.ylabel('fmin')\n",
    "    plt.xlabel('Number of evaluated points')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_result(Y_train,Y_test, Y_train_pred, Y_test_pred):\n",
    "    data_y = np.concatenate((Y_train, Y_test), axis=0)\n",
    "    data_y = data_y.reshape((data_y.shape[0], 1))\n",
    "    sequence_arr = np.arange(1,len(data_y)+1).reshape(len(data_y),1)\n",
    "\n",
    "    pyplot.plot(sequence_arr,data_y, 'o', color='firebrick', label='ground truth')\n",
    "    pyplot.plot(sequence_arr[:len(Y_train)],Y_train_pred,'cornflowerblue',label='pred-train')\n",
    "    pyplot.plot(sequence_arr[len(Y_train):],Y_test_pred,'darkslategray',label='pred-test')\n",
    "    pyplot.legend(loc='best')\n",
    "    pyplot.show()\n",
    "\n",
    "dir_path = 'C:/Users/hcaro/Google Drive/Fall 2018/MLS/Project/DNN_Inference/Experiment Result'\n",
    "bays_dataset = pd.read_csv(dir_path+'/Bays_VDD_40.csv')\n",
    "rand_dataset = pd.read_csv(dir_path+'/Random_vgg_40.csv') #\n",
    "\n",
    "# baysian Optimization Dataset\n",
    "bays_dataset = bays_dataset.values[:,:-2]\n",
    "bays_dataset = bays_dataset.astype('float32')\n",
    "\n",
    "# random Optimization Dataset\n",
    "rand_dataset = rand_dataset.values[:,:-2]\n",
    "rand_dataset = rand_dataset.astype('float32')\n",
    "\n",
    "\n",
    "# exclude CPU & GPU consumption\n",
    "bays_data_x, bays_data_y = bays_dataset[:,:-2], bays_dataset[:,-2:]\n",
    "rand_data_x, rand_data_y = rand_dataset[:,:-2], rand_dataset[:,-2:]\n",
    "\n",
    "test_multi_target_regression(bays_data_x, bays_data_y)\n",
    "#print (\"---------------------------------------------\")\n",
    "test_multi_target_regression(rand_data_x, rand_data_y)\n",
    "\n",
    "\n",
    "#print (\"#########################\")\n",
    "\n",
    "sequence_arr = np.arange(1,len(bays_data_x)+1).reshape(len(bays_data_x),1)\n",
    "n_train = int (len(bays_data_x)*0.8)\n",
    "max_degree = 3\n",
    "degrees = np.arange(2,max_degree+1)\n",
    "pyplot.plot(sequence_arr[n_train:],bays_data_y[n_train:,0],'o', color='navy', linewidth=\"2\", marker='o', label='ground truth')\n",
    "pyplot.plot(sequence_arr[n_train:],bays_data_y[n_train:,0], color='cornflowerblue', linewidth=\"2\", label='ground truth')\n",
    "pred_list = polynomial(bays_data_x, bays_data_y, n_train, init_degree=3, max_degree=max_degree)\n",
    "colors = [\"navy\", \"brown\", \"teal\", \"darkslategray\"]\n",
    "for i in range(len(pred_list)):\n",
    "    pyplot.plot(sequence_arr[n_train:],pred_list[i][:,0],color=colors[i],linewidth=2,label='Degree %d' %degrees[i])\n",
    "pyplot.legend(loc='best')\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.plot(sequence_arr[n_train:],bays_data_y[n_train:,1],'o', color='navy', linewidth=\"2\", marker='o', label='ground truth')\n",
    "pyplot.plot(sequence_arr[n_train:],bays_data_y[n_train:,1], color='cornflowerblue', linewidth=\"2\", label='ground truth')\n",
    "colors = [\"navy\", \"brown\", \"teal\", \"darkslategray\"]\n",
    "for i in range(len(pred_list)):\n",
    "    pyplot.plot(sequence_arr[n_train:],pred_list[i][:,1],color=colors[i],linewidth=2,label='Degree %d' %degrees[i])\n",
    "pyplot.legend(loc='best')\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "plot_min_iteration(bays_data_y, rand_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression_poly.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def test_multi_target_regression_poly(data_x, data_y):\n",
    "    print(len(data_x))\n",
    "    n_train = int (len(data_x)*0.80)\n",
    "    X_train, Y_train = data_x[:n_train], data_y[:n_train]\n",
    "    X_test, Y_test = data_x[n_train:], data_y[n_train:]\n",
    "    references = np.zeros_like(Y_test)\n",
    "    for n in range(2):\n",
    "        rgr = GradientBoostingRegressor(random_state=0)\n",
    "        rgr.fit(X_train, Y_train[:, n])\n",
    "        references[:,n] = rgr.predict(X_test)\n",
    "    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n",
    "    rgr.fit(X_train, Y_train)\n",
    "    \"\" \"\"\"\"\n",
    "    # Create matrix and vectors\n",
    "    X = [[0.44, 0.68], [0.99, 0.23]]\n",
    "    y = [109.85, 155.72]\n",
    "    X_test = [0.49, 0.18]\n",
    "    \"\" \"\"\"[1]\n",
    "    # PolynomialFeatures (prepreprocessing)\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_ = poly.fit_transform(X_train)\n",
    "    X_test_ = poly.fit_transform(X_test)\n",
    "    # Instantiate\n",
    "    lg = LinearRegression()\n",
    "    # Fit\n",
    "    lg.fit(X_, Y_train)\n",
    "    # Obtain coefficients\n",
    "    lg.coef_\n",
    "    # Predict\n",
    "    Y_train_pred = lg.predict(X_train)\n",
    "    Y_test_pred = lg.predict(X_test_)\n",
    "    \"\" \"\"\"\"\n",
    "        Y_train_pred = rgr.predict(X_)\n",
    "        Y_test_pred = rgr.predict(X_test)\n",
    "    \"\" \"\"\"[2]\n",
    "\n",
    "    sequence_arr = np.arange(1,len(data_x)+1).reshape(len(data_x),1)\n",
    "    print(len(Y_test_pred))\n",
    "    print(len(sequence_arr[n_train:]))\n",
    "\n",
    "    plot_result(sequence_arr[:n_train], Y_train[:,0], sequence_arr[n_train:],Y_test[:,0], Y_train_pred[:,0], Y_test_pred[:,0])\n",
    "    plot_result(sequence_arr[:n_train], Y_train[:,1], sequence_arr[n_train:],Y_test[:,1], Y_train_pred[:,1], Y_test_pred[:,1])\n",
    "\n",
    "    rmse_infer = sqrt(mean_squared_error(Y_test_pred[:,0], Y_test[:,0]))\n",
    "    rmse_power = sqrt(mean_squared_error(Y_test_pred[:,1], Y_test[:,1]))\n",
    "    rmse_train_infer = sqrt(mean_squared_error(Y_train_pred[:,0], Y_train[:,0]))\n",
    "    rmse_train_power = sqrt(mean_squared_error(Y_train_pred[:,1], Y_train[:,1]))\n",
    "    print('Train RMSE for Inference Time: %.3f' % rmse_train_infer)\n",
    "    print('Train RMSE for Power Consumption: %.3f' % rmse_train_power)\n",
    "    print('Test RMSE for Inference Time: %.3f' % rmse_infer)\n",
    "    print('Test RMSE for Power Consumption: %.3f' % rmse_power)\n",
    "\n",
    "\n",
    "def test_multi_target_regression(data_x, data_y):\n",
    "    print(len(data_x))\n",
    "    n_train = int (len(data_x)*0.80)\n",
    "    X_train, Y_train = data_x[:n_train], data_y[:n_train]\n",
    "    X_test, Y_test = data_x[n_train:], data_y[n_train:]\n",
    "    references = np.zeros_like(Y_test)\n",
    "\n",
    "    for n in range(2):\n",
    "        rgr = GradientBoostingRegressor(random_state=0)\n",
    "        rgr.fit(X_train, Y_train[:, n])\n",
    "        references[:,n] = rgr.predict(X_test)\n",
    "    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n",
    "    rgr.fit(X_train, Y_train)\n",
    "\n",
    "    Y_train_pred = rgr.predict(X_train)\n",
    "    Y_test_pred = rgr.predict(X_test)\n",
    "    sequence_arr = np.arange(1,len(data_x)+1).reshape(len(data_x),1)\n",
    "    print(len(Y_test_pred))\n",
    "    print(len(sequence_arr[n_train:]))\n",
    "\n",
    "    plot_result(sequence_arr[:n_train], Y_train[:,0], sequence_arr[n_train:],Y_test[:,0], Y_train_pred[:,0], Y_test_pred[:,0])\n",
    "    plot_result(sequence_arr[:n_train], Y_train[:,1], sequence_arr[n_train:],Y_test[:,1], Y_train_pred[:,1], Y_test_pred[:,1])\n",
    "\n",
    "    rmse_infer = sqrt(mean_squared_error(Y_test_pred[:,0], Y_test[:,0]))\n",
    "    rmse_power = sqrt(mean_squared_error(Y_test_pred[:,1], Y_test[:,1]))\n",
    "    rmse_train_infer = sqrt(mean_squared_error(Y_train_pred[:,0], Y_train[:,0]))\n",
    "    rmse_train_power = sqrt(mean_squared_error(Y_train_pred[:,1], Y_train[:,1]))\n",
    "    print('Train RMSE for Inference Time: %.3f' % rmse_train_infer)\n",
    "    print('Train RMSE for Power Consumption: %.3f' % rmse_train_power)\n",
    "    print('Test RMSE for Inference Time: %.3f' % rmse_infer)\n",
    "    print('Test RMSE for Power Consumption: %.3f' % rmse_power)\n",
    "\n",
    "\n",
    "def plot_result(X_train, Y_train, X_test,Y_test, Y_train_pred, Y_test_pred):\n",
    "    pyplot.plot(X_train,Y_train_pred,'b',label='pred-train')\n",
    "    pyplot.plot(X_test,Y_test_pred,'g',label='pred-test')\n",
    "    pyplot.plot(X_train,Y_train,'rx',label='ground truth')\n",
    "    pyplot.plot(X_test,Y_test,'rx')\n",
    "    pyplot.legend(loc='best')\n",
    "    pyplot.show()\n",
    "\n",
    "dir_path = '/home/rick/Project_DNN/Experiment Result'\n",
    "dataset = pd.read_csv(dir_path+'/23Nov_30Samples.csv')\n",
    "dataset = dataset.values[:,:-2]\n",
    "dataset = dataset.astype('float32')\n",
    "data_x, data_y = dataset[:,:-2], dataset[:,-2:]\n",
    "test_multi_target_regression_poly(data_x, data_y)\n",
    "#rmse = sqrt(mean_squared_error(predict, test_y))\n",
    "#print('Test RMSE: %.3f' % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## script.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "9udrp3efOZNd"
   },
   "outputs": [],
   "source": [
    "# possible configuration space\n",
    "$CPU_FREQ = $1\n",
    "$CPU_DIS_CORES = $2\n",
    "$GPU_FREQ = $3\n",
    "$EMC_FREQ = $4\n",
    "\n",
    "sh ./cpu_freq.sh $CPU_FREQ $CPU_DIS_CORES\n",
    "sh ./gpu_freq.sh $GPU_FREQ\n",
    "sh ./emc_freq.sh $EMC_FREQ\n",
    "\n",
    "python run_benchmark.py --bsize 32 --all_growth 1 --mem_frac 0.25\n",
    "\n",
    "# verify configuration\n",
    "cur_gpu_freq=$(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq)\n",
    "cur_cpu_freq=$(cat /sys/devices/57000000.gpu/devfreq/57000000.gpu/cur_freq)\n",
    "cur_emc_freq=$(cat /sys/kernel/debug/tegra_bwmgr/emc_rate)\n",
    "dis_cpu_core_1=$(cat /sys/devices/system/cpu/cpu1/online)\n",
    "dis_cpu_core_2=$(cat /sys/devices/system/cpu/cpu2/online)\n",
    "dis_cpu_core_3=$(cat /sys/devices/system/cpu/cpu3/online)\n",
    "\n",
    "\n",
    "echo \"GPU Frequency: ${cur_gpu_freq}\"\n",
    "echo \"CPU Frequency: ${cur_cpu_freq}\"\n",
    "echo \"EMC Frequency: ${cur_emc_freq}\"\n",
    "echo \"CPU 1 core Status: ${dis_cpu_core_1}\"\n",
    "echo \"CPU 2 core Status: ${dis_cpu_core_2}\"\n",
    "echo \"CPU 3 core Status: ${dis_cpu_core_3}\"\n",
    "#echo \"$host, `date`, checkout,$Time_checkout\" >> log.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme File in our repo will provide more running information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Resnet50 on TX1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./501.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter1](./502.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./503.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./504.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./505.png)"
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.3.1",
  "default_view": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "views": {}
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
